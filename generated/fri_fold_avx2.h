/**
 * fri_fold_avx2.h - AVX2-Vectorized FRI Fold Operations
 *
 * Generated by AmoLean.Vector.CodeGenAVX2
 * AMO-Lean Phase 3
 *
 * This file provides AVX2-vectorized FRI fold operations for the Goldilocks
 * prime field. Operations process 4 field elements in parallel.
 *
 * Requirements:
 *   - x86-64 processor with AVX2 support
 *   - Compile with: clang -O3 -mavx2 or gcc -O3 -mavx2
 *
 * Usage:
 *   #define FIELD_GOLDILOCKS
 *   #include "field_goldilocks.h"
 *   #include "field_goldilocks_avx2.h"
 *   #include "fri_fold_avx2.h"
 */

#ifndef FRI_FOLD_AVX2_H
#define FRI_FOLD_AVX2_H

#include <stddef.h>
#include <stdint.h>
#include <assert.h>

/* Check for x86 architecture */
#if !defined(__x86_64__) && !defined(_M_X64) && !defined(__i386__) && !defined(_M_IX86)
#error "fri_fold_avx2.h requires x86/x86-64 architecture with AVX2"
#endif

/* Ensure AVX2 headers are available */
#ifndef __AVX2__
#error "Compile with -mavx2 to enable AVX2 support"
#endif

/* Requires field_goldilocks_avx2.h */
#ifndef FIELD_GOLDILOCKS_AVX2_H
#error "Include field_goldilocks_avx2.h before fri_fold_avx2.h"
#endif

#ifdef __cplusplus
extern "C" {
#endif

/*===========================================================================
 * AVX2-Vectorized FRI Fold Operations
 *===========================================================================*/

/**
 * AVX2-vectorized FRI fold: out[i] = even[i] + alpha * odd[i]
 *
 * PROOF_ANCHOR: fri_fold_avx2
 * Preconditions:
 *   - n is a multiple of 4 (for optimal performance)
 *   - even, odd, out point to arrays of at least n elements
 *   - For aligned access: pointers must be 32-byte aligned
 *   - out does not alias even or odd
 * Postconditions:
 *   - forall i in [0, n): out[i] == (even[i] + alpha * odd[i]) mod p
 * Performance:
 *   - Processes 4 elements per iteration
 *   - Scalar tail loop for n % 4 remainder
 */
static inline void fri_fold_avx2(
    size_t n,
    const uint64_t* restrict even,
    const uint64_t* restrict odd,
    uint64_t* restrict out,
    uint64_t alpha
) {
#ifdef DEBUG
    assert(even != NULL && "even is null");
    assert(odd != NULL && "odd is null");
    assert(out != NULL && "out is null");
    assert(out != even && "out aliases even");
    assert(out != odd && "out aliases odd");
#endif

    /* Broadcast alpha to all 4 lanes */
    __m256i alpha_vec = goldilocks_avx2_broadcast(alpha);

    /* Vector count (multiple of 4) */
    size_t vec_count = n & ~3ULL;

    /* Main vectorized loop */
    for (size_t i = 0; i < vec_count; i += 4) {
        __m256i v_even = goldilocks_avx2_loadu(&even[i]);
        __m256i v_odd = goldilocks_avx2_loadu(&odd[i]);
        __m256i v_out = goldilocks_avx2_fri_fold(v_even, v_odd, alpha_vec);
        goldilocks_avx2_storeu(&out[i], v_out);
    }

    /* Scalar tail loop for remainder */
    for (size_t i = vec_count; i < n; i++) {
        out[i] = goldilocks_add(even[i], goldilocks_mul(alpha, odd[i]));
    }
}

/**
 * AVX2-vectorized FRI layer fold: output[i] = input[2*i] + alpha * input[2*i+1]
 *
 * PROOF_ANCHOR: fri_fold_layer_avx2
 * Preconditions:
 *   - n > 0, n is the output size (input has 2*n elements)
 *   - n should be a multiple of 4 for optimal performance
 *   - input has 2*n elements, output has n elements
 *   - output does not alias input
 * Postconditions:
 *   - forall i in [0, n): output[i] == (input[2*i] + alpha * input[2*i+1]) mod p
 * Note:
 *   - Requires gather operations for non-contiguous access pattern
 */
static inline void fri_fold_layer_avx2(
    size_t n,
    const uint64_t* restrict input,
    uint64_t* restrict output,
    uint64_t alpha
) {
#ifdef DEBUG
    assert(n > 0 && "n must be positive");
    assert(input != NULL && "input is null");
    assert(output != NULL && "output is null");
    assert(output != input && "output aliases input");
#endif

    __m256i alpha_vec = goldilocks_avx2_broadcast(alpha);

    size_t vec_count = n & ~3ULL;

    /* Main vectorized loop */
    for (size_t i = 0; i < vec_count; i += 4) {
        /* Gather even indices: input[2*i], input[2*i+2], input[2*i+4], input[2*i+6] */
        __m256i v_even = _mm256_set_epi64x(
            input[2*(i+3)], input[2*(i+2)], input[2*(i+1)], input[2*i]
        );

        /* Gather odd indices: input[2*i+1], input[2*i+3], input[2*i+5], input[2*i+7] */
        __m256i v_odd = _mm256_set_epi64x(
            input[2*(i+3)+1], input[2*(i+2)+1], input[2*(i+1)+1], input[2*i+1]
        );

        __m256i v_out = goldilocks_avx2_fri_fold(v_even, v_odd, alpha_vec);
        goldilocks_avx2_storeu(&output[i], v_out);
    }

    /* Scalar tail loop */
    for (size_t i = vec_count; i < n; i++) {
        output[i] = goldilocks_add(input[2*i], goldilocks_mul(alpha, input[2*i+1]));
    }
}

/*===========================================================================
 * Batch Operations for Large Arrays
 *===========================================================================*/

/**
 * AVX2-vectorized batch FRI fold for large arrays
 *
 * Optimized version with software prefetching for large datasets.
 *
 * @param out Output array (n elements)
 * @param even Input even coefficients (n elements)
 * @param odd Input odd coefficients (n elements)
 * @param n Number of elements
 * @param alpha Folding challenge
 */
static inline void fri_fold_avx2_batch(
    uint64_t* restrict out,
    const uint64_t* restrict even,
    const uint64_t* restrict odd,
    size_t n,
    uint64_t alpha
) {
    __m256i alpha_vec = goldilocks_avx2_broadcast(alpha);

    size_t vec_count = n & ~15ULL;  /* Process 16 at a time for prefetching */

    /* Main loop with prefetching */
    for (size_t i = 0; i < vec_count; i += 16) {
        /* Prefetch next cache lines */
        _mm_prefetch((const char*)&even[i + 64], _MM_HINT_T0);
        _mm_prefetch((const char*)&odd[i + 64], _MM_HINT_T0);

        /* Process 4 vectors (16 elements) */
        for (size_t j = 0; j < 16; j += 4) {
            __m256i v_even = goldilocks_avx2_loadu(&even[i + j]);
            __m256i v_odd = goldilocks_avx2_loadu(&odd[i + j]);
            __m256i v_out = goldilocks_avx2_fri_fold(v_even, v_odd, alpha_vec);
            goldilocks_avx2_storeu(&out[i + j], v_out);
        }
    }

    /* Handle remaining elements */
    for (size_t i = vec_count; i < n; i += 4) {
        size_t count = (n - i < 4) ? n - i : 4;
        if (count == 4) {
            __m256i v_even = goldilocks_avx2_loadu(&even[i]);
            __m256i v_odd = goldilocks_avx2_loadu(&odd[i]);
            __m256i v_out = goldilocks_avx2_fri_fold(v_even, v_odd, alpha_vec);
            goldilocks_avx2_storeu(&out[i], v_out);
        } else {
            /* Scalar tail */
            for (size_t j = 0; j < count; j++) {
                out[i + j] = goldilocks_add(even[i + j],
                    goldilocks_mul(alpha, odd[i + j]));
            }
        }
    }
}

/*===========================================================================
 * Helper Operations for Poseidon2
 *===========================================================================*/

/**
 * AVX2-vectorized 4x4 MDS matrix-vector multiplication
 *
 * Computes: out = M * in, where M is a 4x4 MDS matrix
 *
 * PROOF_ANCHOR: mds_4x4_avx2
 * Preconditions:
 *   - in, out are 4-element arrays (32-byte aligned preferred)
 *   - matrix is row-major 4x4 (16 elements, 128-byte aligned preferred)
 * Postconditions:
 *   - out[i] = sum(j=0..3) matrix[i*4+j] * in[j] mod p
 */
static inline void mds_4x4_avx2(
    const uint64_t* matrix,  /* 4x4 matrix, row-major */
    const uint64_t* in,       /* 4-element input vector */
    uint64_t* out             /* 4-element output vector */
) {
    /* Load input vector */
    __m256i v_in = goldilocks_avx2_loadu(in);

    for (int row = 0; row < 4; row++) {
        /* Load matrix row */
        __m256i v_row = goldilocks_avx2_loadu(&matrix[row * 4]);

        /* Compute dot product using scalar reduction */
        out[row] = goldilocks_avx2_dot4(v_row, v_in);
    }
}

/**
 * AVX2-vectorized element-wise S-box (x^7 for Goldilocks)
 *
 * Applies the S-box transformation to 4 elements in parallel.
 */
static inline __m256i goldilocks_avx2_sbox(__m256i x) {
    /* x^2 */
    __m256i x2 = goldilocks_avx2_square(x);
    /* x^4 */
    __m256i x4 = goldilocks_avx2_square(x2);
    /* x^6 = x^4 * x^2 */
    __m256i x6 = goldilocks_avx2_mul(x4, x2);
    /* x^7 = x^6 * x */
    return goldilocks_avx2_mul(x6, x);
}

#ifdef __cplusplus
}
#endif

#endif /* FRI_FOLD_AVX2_H */
